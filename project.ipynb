{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import jieba\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "import requests\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.oxml.ns import qn\n",
    "from docx.shared import Inches, RGBColor\n",
    "from PIL import Image\n",
    "import os\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query source and output result path\n",
    "student_id = \"M11207408\"\n",
    "query_path = f\"./queries.txt\"\n",
    "results_search = \"./results_search\"\n",
    "results_sorted = \"./results_sorted\"\n",
    "\n",
    "# Web scraping target URL\n",
    "search_url_m = \"https://www.momoshop.com.tw/search/searchShop.jsp?keyword=\"\n",
    "search_url_p = \"https://24h.pchome.com.tw/search/?q=\"\n",
    "\n",
    "# Scraping parameter settings\n",
    "short_time_sleep = 1\n",
    "medium_time_sleep = 3\n",
    "long_time_sleep = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries from file\n",
    "def read_queries(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "# Check if the webpage is accessible\n",
    "def check_access(driver):\n",
    "    try:\n",
    "        text = driver.find_element(By.XPATH, '/html/body/h1')\n",
    "        if text.text == 'Access Denied':\n",
    "            return False\n",
    "        else: \n",
    "            return True\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "# Get all items from pages (momo)\n",
    "def get_all_items_m(driver):\n",
    "    items = pd.DataFrame()\n",
    "    try:\n",
    "        length = 0\n",
    "        i = 0\n",
    "        while length < 100:\n",
    "            item = driver.find_elements(By.XPATH, \"//div[@class = 'goodsUrl']\")\n",
    "            temp = extract_item_info_m(item)\n",
    "            items = pd.concat([items, temp], ignore_index=True)\n",
    "            length = len(items)  # 更新長度\n",
    "            \n",
    "            if length >= 100:  # 已獲取足夠資料，提前結束\n",
    "                break\n",
    "            \n",
    "            # 翻頁邏輯\n",
    "            try:\n",
    "                nextpage = driver.find_elements(By.XPATH, \"//div[@class = 'page-btn page-next']\")\n",
    "                if nextpage:  # 確保找到下一頁按鈕\n",
    "                    nextpage[1].click()\n",
    "                    time.sleep(long_time_sleep)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    print(\"No more pages to click.\")\n",
    "                    break  # 若沒有下一頁則結束迴圈\n",
    "            except Exception as e:\n",
    "                print(f\"Click fail: {e}\")\n",
    "                break  # 若翻頁出錯則結束迴圈\n",
    "            \n",
    "            if i > 10:  # 防止過多翻頁\n",
    "                break\n",
    "\n",
    "        items.drop_duplicates(inplace=True, subset=['momo_name'])\n",
    "        items = items.head(100)  # 確保只返回前100筆資料\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(f\"No items found: {e}\")\n",
    "\n",
    "\n",
    "# Extract all item informations to a dataframe   \n",
    "def extract_item_info_m(items):\n",
    "    print(\"Extracting item information...\")\n",
    "    data = []\n",
    "    for i, item in enumerate(items):\n",
    "        try:\n",
    "            item_name = item.find_element(By.XPATH, 'div[2]/h3').text\n",
    "            item_url = item.find_element(By.XPATH, 'div[1]/div/div/div/a').get_attribute('href')\n",
    "            item_price = item.find_element(By.XPATH, 'div[3]/div[2]/span/b').text\n",
    "            item_pic_url = item.find_element(By.XPATH, \"div[1]/div/div[1]/div[1]/a/picture/img\").get_attribute('src')\n",
    "\n",
    "            \n",
    "            data.append({\n",
    "                'momo_name': item_name,\n",
    "                'momo_price': item_price,\n",
    "                'momo_url': item_url,\n",
    "                'momo_pic': item_pic_url\n",
    "            })\n",
    "        except:\n",
    "            print(f\"Error extracting item {i}.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def scroll_p(driver):\n",
    "    # scroll\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    return new_height != last_height\n",
    "\n",
    "# Get all items from pages (PChome)\n",
    "def get_all_items_p(driver):\n",
    "    try:\n",
    "        items = driver.find_elements(By.XPATH, \"/html/body/div[1]/div[2]/div[1]/div[2]/div[1]/div[2]/dl\")\n",
    "        data = []\n",
    "        seen_names = set()\n",
    "\n",
    "        while len(data) < 100:\n",
    "            # 重新獲取當前項目\n",
    "            items = driver.find_elements(By.XPATH, \"/html/body/div[1]/div[2]/div[1]/div[2]/div[1]/div[2]/dl\")\n",
    "            for item in items:\n",
    "                try:\n",
    "                    item_name = item.find_element(By.XPATH, 'dd[2]/h5/a').text\n",
    "                    item_url = item.find_element(By.XPATH, 'dd[2]/h5/a').get_attribute('href')\n",
    "                    item_price = item.find_element(By.XPATH, 'dd[3]/ul[1]/li/span/span').text  # on sale.\n",
    "                    try: #R18網頁抓不到圖片\n",
    "                        item_pic_url = item.find_element(By.XPATH, 'dd[1]/a/img').get_attribute('src')\n",
    "                    except:\n",
    "                        item_pic_url = \"no picture\"\n",
    "\n",
    "                    if item_name not in seen_names:\n",
    "\n",
    "                        \n",
    "                        seen_names.add(item_name)\n",
    "                        data.append({\n",
    "                            'PChome_name': item_name,\n",
    "                            'PChome_price': item_price,\n",
    "                            'PChome_url': item_url,\n",
    "                            'PChome_pic': item_pic_url\n",
    "                        })\n",
    "                except:\n",
    "                    print(f\"Error extracting item.\")\n",
    "\n",
    "            # 檢查是否達到 100 筆資料\n",
    "            if len(data) >= 100:\n",
    "                break\n",
    "\n",
    "            # 滾動頁面\n",
    "            if not scroll_p(driver):\n",
    "                break\n",
    "        data = pd.DataFrame(data).head(100)\n",
    "        return data  # 回傳收集到的資料\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 運動鞋 have already been scraped. Skipping...\n",
      "\n",
      "Results for 3_5mm 轉 usb have already been scraped. Skipping...\n",
      "\n",
      "Results for amd_迷你主機 have already been scraped. Skipping...\n",
      "\n",
      "Results for inktank_115 have already been scraped. Skipping...\n",
      "\n",
      "Results for iphone 音源線 have already been scraped. Skipping...\n",
      "\n",
      "Results for mio_行車紀錄器支架 have already been scraped. Skipping...\n",
      "\n",
      "Results for papago_導航機 have already been scraped. Skipping...\n",
      "\n",
      "Results for rtx 顯示卡 have already been scraped. Skipping...\n",
      "\n",
      "Results for switch_底座 have already been scraped. Skipping...\n",
      "\n",
      "Results for zenpower_10050 have already been scraped. Skipping...\n",
      "\n",
      "Results for 大聖誕樹 have already been scraped. Skipping...\n",
      "\n",
      "Results for 斗篷雨衣 have already been scraped. Skipping...\n",
      "\n",
      "Results for 交換禮物 have already been scraped. Skipping...\n",
      "\n",
      "Results for 行動電源 have already been scraped. Skipping...\n",
      "\n",
      "Results for 兒童毛帽 have already been scraped. Skipping...\n",
      "\n",
      "Results for 便攜式咖啡機 have already been scraped. Skipping...\n",
      "\n",
      "Results for 倍潔雅 have already been scraped. Skipping...\n",
      "\n",
      "Results for 馬玉山紅藜麥 have already been scraped. Skipping...\n",
      "\n",
      "Results for 莊園巧克力 have already been scraped. Skipping...\n",
      "\n",
      "Results for 發電機 have already been scraped. Skipping...\n",
      "\n",
      "Results for 感應垃圾桶 have already been scraped. Skipping...\n",
      "\n",
      "Results for 電暖器 have already been scraped. Skipping...\n",
      "\n",
      "Results for 樂高_71043 have already been scraped. Skipping...\n",
      "\n",
      "Results for 燈串 have already been scraped. Skipping...\n",
      "\n",
      "Results for 麵包 have already been scraped. Skipping...\n",
      "\n",
      "Results for 冰糖燕窩 have already been scraped. Skipping...\n",
      "\n",
      "Results for 國際牌 星光 have already been scraped. Skipping...\n",
      "\n",
      "Results for 安撫娃娃 have already been scraped. Skipping...\n",
      "\n",
      "Results for 微波爐 have already been scraped. Skipping...\n",
      "\n",
      "Results for 教學無線麥克風 have already been scraped. Skipping...\n",
      "\n",
      "Results for 水波爐 have already been scraped. Skipping...\n",
      "\n",
      "Results for 無線鍵盤滑鼠組 have already been scraped. Skipping...\n",
      "\n",
      "Results for 磁磚清潔 have already been scraped. Skipping...\n",
      "\n",
      "Results for 舒潔 拉拉 have already been scraped. Skipping...\n",
      "\n",
      "Results for 軍毯 have already been scraped. Skipping...\n",
      "\n",
      "Results for 雪q餅 have already been scraped. Skipping...\n",
      "\n",
      "Results for 飛機杯 have already been scraped. Skipping...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the webpage\n",
    "driver = webdriver.Chrome()\n",
    "time.sleep(short_time_sleep)\n",
    "\n",
    "# Main scraping process\n",
    "queries = read_queries(query_path)\n",
    "for query in queries:\n",
    "    # Get all csv files in the results folder, if exists pass the query\n",
    "    csv_files = [f for f in os.listdir(results_search) if f.endswith('.csv')]\n",
    "    search_string = query\n",
    "    all_contain_string = any(search_string in file_name for file_name in csv_files)\n",
    "    if all_contain_string:\n",
    "        print(f\"Results for {query} have already been scraped. Skipping...\\n\")\n",
    "        continue\n",
    "    \n",
    "    # Search for the query(momo)\n",
    "    driver.get(search_url_m + query)\n",
    "    time.sleep(medium_time_sleep)\n",
    "    status = check_access(driver)\n",
    "    if status:\n",
    "        print(f\"Start scraping {query}...\")\n",
    "    else:\n",
    "        print(f\"Some error occurred while scraping {query}.\")\n",
    "        continue\n",
    "    # Process the items(momo)\n",
    "    items_df_m = get_all_items_m(driver)\n",
    "   \n",
    "    # Search for the query(PChome)\n",
    "    driver.get(search_url_p + query)\n",
    "    time.sleep(medium_time_sleep)\n",
    "    status = check_access(driver)\n",
    "    if status:\n",
    "        print(f\"Start scraping {query}...\")\n",
    "    else:\n",
    "        print(f\"Some error occurred while scraping {query}.\")\n",
    "        continue\n",
    "    # Process the items(PChome)\n",
    "    items_df_p = get_all_items_p(driver)\n",
    "\n",
    "    # 清除空白值\n",
    "    items_df_m_cleaned = items_df_m.dropna().reset_index(drop=True)\n",
    "    items_df_p_cleaned = items_df_p.dropna().reset_index(drop=True)\n",
    "    # 合併 DataFrame\n",
    "    items_df = pd.concat([items_df_m_cleaned, items_df_p_cleaned], axis=1)\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    file_path = os.path.join(results_search, f\"{query}.csv\")\n",
    "    items_df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Results for {query} have been saved to {file_path}\")\n",
    "    time.sleep(long_time_sleep + random.random() * 10)\n",
    "    print(\"Sleeping for a while...\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    \n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtion for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取型號的函數\n",
    "def extract_model(name):\n",
    "    pattern = r'([A-Za-z0-9\\-.]+)'\n",
    "    matches = re.findall(pattern, name)\n",
    "    matches = [match for match in matches if ' ' not in match and match.strip()]\n",
    "    return ' '.join(matches)\n",
    "\n",
    "# 定義簡單的分詞器\n",
    "def jieba_tokenizer(text):\n",
    "    tokens = jieba.lcut(text, cut_all=False)\n",
    "    stop_words = ['【', '】', '/', '~', '＊', '、', '（', '）', '+', '‧', ' ', '', '-', '&']\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) >= 2] #至少兩個字\n",
    "    return tokens\n",
    "\n",
    "# Read queries from file\n",
    "def read_queries(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 運動鞋 have been saved\n",
      "Results for 3_5mm 轉 usb have been saved\n",
      "Results for amd_迷你主機 have been saved\n",
      "inktank_115.csv 有欄位為空\n",
      "Results for iphone 音源線 have been saved\n",
      "Results for mio_行車紀錄器支架 have been saved\n",
      "Results for papago_導航機 have been saved\n",
      "Results for rtx 顯示卡 have been saved\n",
      "Results for switch_底座 have been saved\n",
      "zenpower_10050.csv 為空\n",
      "Results for 大聖誕樹 have been saved\n",
      "Results for 斗篷雨衣 have been saved\n",
      "Results for 交換禮物 have been saved\n",
      "Results for 行動電源 have been saved\n",
      "Results for 兒童毛帽 have been saved\n",
      "Results for 便攜式咖啡機 have been saved\n",
      "Results for 倍潔雅 have been saved\n",
      "Results for 馬玉山紅藜麥 have been saved\n",
      "Results for 莊園巧克力 have been saved\n",
      "Results for 發電機 have been saved\n",
      "Results for 感應垃圾桶 have been saved\n",
      "Results for 電暖器 have been saved\n",
      "Results for 樂高_71043 have been saved\n",
      "Results for 燈串 have been saved\n",
      "Results for 麵包 have been saved\n",
      "Results for 冰糖燕窩 have been saved\n",
      "Results for 國際牌 星光 have been saved\n",
      "Results for 安撫娃娃 have been saved\n",
      "Results for 微波爐 have been saved\n",
      "Results for 教學無線麥克風 have been saved\n",
      "Results for 水波爐 have been saved\n",
      "Results for 無線鍵盤滑鼠組 have been saved\n",
      "Results for 磁磚清潔 have been saved\n",
      "舒潔 拉拉.csv 有欄位為空\n",
      "軍毯.csv 有欄位為空\n",
      "Results for 雪q餅 have been saved\n",
      "Results for 飛機杯 have been saved\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "        try:\n",
    "            # 讀取商品數據的 CSV 檔案\n",
    "            df = pd.read_csv(f'{results_search}/{query}.csv')  # 替換為你的檔案路徑\n",
    "            # 檢查 DataFrame 是否為空\n",
    "        except:\n",
    "            print(f\"{query}.csv 為空\") #兩個網站都沒有搜尋結果\n",
    "            continue  # 跳過這次迴圈，繼續下一個\n",
    "        \n",
    "        # 預處理\n",
    "        try:\n",
    "            df['momo_name'] = df['momo_name'].str.lower()\n",
    "            df['PChome_name'] = df['PChome_name'].str.lower()\n",
    "            df['momo_name'] = df['momo_name'].fillna('').str.replace('-', '', regex=False)\n",
    "            df['PChome_name'] = df['PChome_name'].fillna('').str.replace('-', '', regex=False)\n",
    "        except:\n",
    "            print(f'{query}.csv 有欄位為空') #其中一個網站沒有搜尋結果\n",
    "            continue\n",
    "\n",
    "        # 使用 jieba 進行分詞\n",
    "        df['momo_tokens'] = df['momo_name'].apply(jieba_tokenizer)\n",
    "        df['PChome_tokens'] = df['PChome_name'].apply(jieba_tokenizer)\n",
    "\n",
    "        # 建立 BM25 模型以進行比較\n",
    "        bm25 = BM25Okapi(df['momo_tokens'].tolist())\n",
    "\n",
    "        matched_products = []\n",
    "        used_pchome_names = set()  # 用於追蹤已匹配的 PChome 商品名稱\n",
    "\n",
    "        for pc_name, pc_tokens, pc_price, pc_url, pc_pic in zip(df['PChome_name'], df['PChome_tokens'], df['PChome_price'], df['PChome_url'], df['PChome_pic']):\n",
    "            if pc_name in used_pchome_names:\n",
    "                continue\n",
    "            \n",
    "            scores = bm25.get_scores(pc_tokens)\n",
    "            \n",
    "            best_match = None\n",
    "            best_score = -1\n",
    "\n",
    "            for i, score in enumerate(scores):\n",
    "                if len(df['momo_tokens'].iloc[i]) > 18:\n",
    "                    score *= 0.8  # 名稱內token太多降低分數\n",
    "\n",
    "                if score > best_score and score > 1:\n",
    "                    best_score = score\n",
    "                    best_match = {\n",
    "                        'PChome_name': pc_name,\n",
    "                        'PChome_price': pc_price,\n",
    "                        'PChome_url': pc_url,\n",
    "                        'PChome_pic': pc_pic,      \n",
    "                        'momo_name': df['momo_name'].iloc[i],\n",
    "                        'momo_price': df['momo_price'].iloc[i],\n",
    "                        'momo_url': df['momo_url'].iloc[i],\n",
    "                        'momo_pic':df['momo_pic'].iloc[i],    \n",
    "                        'score': score\n",
    "                    }\n",
    "\n",
    "            if best_match:\n",
    "                matched_products.append(best_match)\n",
    "                used_pchome_names.add(best_match['PChome_name'])\n",
    "\n",
    "\n",
    "        result_df = pd.DataFrame(matched_products)\n",
    "        result_df.sort_values(by='score', ascending=False, inplace=True)\n",
    "\n",
    "        # 將結果寫入 CSV 檔案，使用 utf-8-sig 編碼\n",
    "        result_df.to_csv(f'{results_sorted}/{query}_matched_products.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"Results for {query} have been saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for csv2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_convert_image(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            # 檢查是否是 WebP 格式\n",
    "            if img.format == 'WEBP':\n",
    "                save_path = save_path.replace(\".jpg\", \".png\")  # 轉為 PNG 格式\n",
    "                img = img.convert(\"RGB\")  # 將 WebP 轉換為 RGB 格式\n",
    "                img.save(save_path, 'PNG')\n",
    "            else:\n",
    "                with open(save_path, 'wb') as out_file:\n",
    "                    out_file.write(response.content)\n",
    "            return save_path\n",
    "        else:\n",
    "            print(f\"Failed to download {url}, status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def safe_add_image(cell, image_path, width):\n",
    "    try:\n",
    "        cell.paragraphs[0].add_run().add_picture(image_path, width=width)\n",
    "    except Exception as e:\n",
    "        cell.text = 'No Image'\n",
    "        print(f\"Failed to add image {image_path}: {e}\")\n",
    "\n",
    "# 添加超連結\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    為段落添加一個帶有超連結的文字\n",
    "    :param paragraph: 段落對象\n",
    "    :param url: 超連結的網址 (字串)\n",
    "    :param text: 顯示在段落中的文字 (字串)\n",
    "    \"\"\"\n",
    "    # 建立超連結的 rId\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # 創建 <w:hyperlink> 標籤\n",
    "    hyperlink = OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(qn('r:id'), r_id)\n",
    "\n",
    "    # 創建 <w:r> 標籤\n",
    "    run = OxmlElement('w:r')\n",
    "    run_properties = OxmlElement('w:rPr')\n",
    "\n",
    "    # 設置超連結文字為藍色並加下劃線\n",
    "    color = OxmlElement('w:color')\n",
    "    color.set(qn('w:val'), \"0000FF\")  # 設置為藍色\n",
    "    run_properties.append(color)\n",
    "\n",
    "    underline = OxmlElement('w:u')\n",
    "    underline.set(qn('w:val'), 'single')  # 添加下劃線\n",
    "    run_properties.append(underline)\n",
    "\n",
    "    run.append(run_properties)\n",
    "\n",
    "    # 添加顯示文字\n",
    "    text_element = OxmlElement('w:t')\n",
    "    text_element.text = text\n",
    "    run.append(text_element)\n",
    "\n",
    "    # 將 run 加入 hyperlink\n",
    "    hyperlink.append(run)\n",
    "\n",
    "    # 將 hyperlink 加入段落\n",
    "    paragraph._element.append(hyperlink)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./word/運動鞋.docx already exists. Skipping...\n",
      "./word/3_5mm 轉 usb.docx already exists. Skipping...\n",
      "./word/amd_迷你主機.docx already exists. Skipping...\n",
      "No inktank_115 file found, skipping...\n",
      "./word/iphone 音源線.docx already exists. Skipping...\n",
      "./word/mio_行車紀錄器支架.docx already exists. Skipping...\n",
      "./word/papago_導航機.docx already exists. Skipping...\n",
      "./word/rtx 顯示卡.docx already exists. Skipping...\n",
      "./word/switch_底座.docx already exists. Skipping...\n",
      "No zenpower_10050 file found, skipping...\n",
      "./word/大聖誕樹.docx already exists. Skipping...\n",
      "./word/斗篷雨衣.docx already exists. Skipping...\n",
      "./word/交換禮物.docx already exists. Skipping...\n",
      "./word/行動電源.docx already exists. Skipping...\n",
      "./word/兒童毛帽.docx already exists. Skipping...\n",
      "./word/便攜式咖啡機.docx already exists. Skipping...\n",
      "./word/倍潔雅.docx already exists. Skipping...\n",
      "Item 馬玉山紅藜麥 converted and saved to ./word/馬玉山紅藜麥.docx\n",
      "Item 莊園巧克力 converted and saved to ./word/莊園巧克力.docx\n",
      "Item 發電機 converted and saved to ./word/發電機.docx\n",
      "./word/感應垃圾桶.docx already exists. Skipping...\n",
      "./word/電暖器.docx already exists. Skipping...\n",
      "./word/樂高_71043.docx already exists. Skipping...\n",
      "./word/燈串.docx already exists. Skipping...\n",
      "./word/麵包.docx already exists. Skipping...\n",
      "./word/冰糖燕窩.docx already exists. Skipping...\n",
      "./word/國際牌 星光.docx already exists. Skipping...\n",
      "./word/安撫娃娃.docx already exists. Skipping...\n",
      "./word/微波爐.docx already exists. Skipping...\n",
      "./word/教學無線麥克風.docx already exists. Skipping...\n",
      "./word/水波爐.docx already exists. Skipping...\n",
      "./word/無線鍵盤滑鼠組.docx already exists. Skipping...\n",
      "./word/磁磚清潔.docx already exists. Skipping...\n",
      "No 舒潔 拉拉 file found, skipping...\n",
      "No 軍毯 file found, skipping...\n",
      "./word/雪q餅.docx already exists. Skipping...\n",
      "./word/飛機杯.docx already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_dir = \"./image\"\n",
    "doc = Document()\n",
    "doc.add_heading('Product Comparison', 0)\n",
    "os.makedirs(image_dir, exist_ok=True)  # 確保圖片目錄存在\n",
    "\n",
    "for query in queries:\n",
    "    word_output_path = f'./word/{query}.docx'\n",
    "    \n",
    "    # 檢查 Word 檔案是否已存在\n",
    "    if os.path.exists(word_output_path):\n",
    "        print(f\"{word_output_path} already exists. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # 讀取csv檔案\n",
    "    try:\n",
    "        df = pd.read_csv(f'{results_sorted}/{query}_matched_products.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No {query} file found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    doc = Document()\n",
    "    doc.add_heading('Product Comparison', 0)\n",
    "\n",
    "    # 添加表格\n",
    "    table = doc.add_table(rows=1, cols=6)\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    hdr_cells[0].text = 'PChome Name'\n",
    "    hdr_cells[1].text = 'PChome Price'\n",
    "    hdr_cells[2].text = 'PChome Image'\n",
    "    hdr_cells[3].text = 'Momo Name'\n",
    "    hdr_cells[4].text = 'Momo Price'\n",
    "    hdr_cells[5].text = 'Momo Image'\n",
    "\n",
    "    # 修改下載與插入圖片的部分\n",
    "    for idx, row in df.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "\n",
    "        # 插入 PChome 名稱及其超連結\n",
    "        pchome_paragraph = row_cells[0].paragraphs[0]\n",
    "        add_hyperlink(pchome_paragraph, row['PChome_url'], str(row['PChome_name']))\n",
    "        row_cells[1].text = str(row['PChome_price'])\n",
    "\n",
    "        # 下載並插入 PChome 圖片\n",
    "        pchome_image_path = os.path.join(image_dir, f\"pchome_image_{idx}.jpg\")\n",
    "        if download_and_convert_image(row['PChome_pic'], pchome_image_path):\n",
    "            safe_add_image(row_cells[2], pchome_image_path, width=Inches(1.0))\n",
    "        else:\n",
    "            row_cells[2].text = 'No Image'\n",
    "\n",
    "        # 插入 Momo 名稱及其超連結\n",
    "        momo_paragraph = row_cells[3].paragraphs[0]\n",
    "        add_hyperlink(momo_paragraph, row['momo_url'], str(row['momo_name']))\n",
    "        row_cells[4].text = str(row['momo_price'])\n",
    "\n",
    "        # 下載並插入 Momo 圖片\n",
    "        momo_image_path = os.path.join(image_dir, f\"momo_image_{idx}.png\")\n",
    "        if download_and_convert_image(row['momo_pic'], momo_image_path):\n",
    "            safe_add_image(row_cells[5], momo_image_path, width=Inches(1.0))\n",
    "        else:\n",
    "            row_cells[5].text = 'No Image'\n",
    "\n",
    "    # 保存 Word\n",
    "    doc.save(word_output_path)\n",
    "    print(f\"Item {query} converted and saved to {word_output_path}\")\n",
    "    table._element.clear()  # 清空表格的所有內容"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
